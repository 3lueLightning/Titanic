{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle\n",
    "import sagemaker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEE HOW TO IMPORT parameterdict FROM PICKLED FILE WITHOUT HAVING TO REDIFINE THE parameterdict CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AIM IS TO REMOVE THIS CELL\n",
    "class parameterdict(dict):\n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "        \n",
    "    def store(self):\n",
    "        with open(self.file_name, 'ab') as dbfile:\n",
    "            pickle.dump(self, dbfile)                      \n",
    "        return\n",
    "    \n",
    "    def load(self):  \n",
    "        with open(self.file_name, 'rb') as dbfile:     \n",
    "            db = pickle.load(dbfile)  \n",
    "        return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('titanic_parameters.pkl', 'rb') as dbfile:\n",
    "    PARAMETERS = pickle.load(dbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my_region': 'eu-west-1',\n",
       " 'bucket_name': 's3.sagemaker.end2endtitanic.7ededbe5308646a3adbc'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'train' notebook we trained an XGBoost model, now by using the 'attach' function we can attach the model artifacts in order to instanciate a new XGBoost model that we can deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-15 18:59:08 Starting - Preparing the instances for training\n",
      "2019-07-15 18:59:08 Downloading - Downloading input data\n",
      "2019-07-15 18:59:08 Training - Training image download completed. Training in progress.\n",
      "2019-07-15 18:59:08 Uploading - Uploading generated training model\n",
      "2019-07-15 18:59:08 Completed - Training job completed\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[2019-07-15:18:58:57:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[31m[2019-07-15:18:58:57:INFO] File size need to be processed in the node: 0.02mb. Available memory size in the node: 8464.18mb\u001b[0m\n",
      "\u001b[31m[2019-07-15:18:58:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m[18:58:57] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[31m[18:58:57] 668x7 matrix with 4676 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[31m[2019-07-15:18:58:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m[18:58:57] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[31m[18:58:57] 223x7 matrix with 1561 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[0]#011train-error:0.170659#011validation-error:0.242152\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[1]#011train-error:0.166168#011validation-error:0.251121\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[2]#011train-error:0.169162#011validation-error:0.237668\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[3]#011train-error:0.149701#011validation-error:0.210762\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[4]#011train-error:0.137725#011validation-error:0.210762\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[5]#011train-error:0.134731#011validation-error:0.206278\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 36 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[6]#011train-error:0.134731#011validation-error:0.206278\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[7]#011train-error:0.131737#011validation-error:0.206278\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[8]#011train-error:0.131737#011validation-error:0.215247\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[9]#011train-error:0.131737#011validation-error:0.215247\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[10]#011train-error:0.124251#011validation-error:0.215247\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[11]#011train-error:0.122754#011validation-error:0.206278\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[12]#011train-error:0.122754#011validation-error:0.206278\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[13]#011train-error:0.124251#011validation-error:0.197309\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[14]#011train-error:0.125749#011validation-error:0.206278\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[15]#011train-error:0.124251#011validation-error:0.201794\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[16]#011train-error:0.124251#011validation-error:0.206278\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[17]#011train-error:0.122754#011validation-error:0.197309\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[18]#011train-error:0.116766#011validation-error:0.206278\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 26 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[31m[19]#011train-error:0.116766#011validation-error:0.206278\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[20]#011train-error:0.115269#011validation-error:0.206278\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[21]#011train-error:0.115269#011validation-error:0.210762\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[22]#011train-error:0.115269#011validation-error:0.210762\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[23]#011train-error:0.115269#011validation-error:0.210762\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[24]#011train-error:0.115269#011validation-error:0.210762\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[25]#011train-error:0.115269#011validation-error:0.210762\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[26]#011train-error:0.115269#011validation-error:0.210762\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[27]#011train-error:0.115269#011validation-error:0.210762\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[28]#011train-error:0.115269#011validation-error:0.210762\u001b[0m\n",
      "\u001b[31m[18:58:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 20 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[29]#011train-error:0.115269#011validation-error:0.210762\u001b[0m\n",
      "Billable seconds: 51\n"
     ]
    }
   ],
   "source": [
    "xgb_model = sagemaker.estimator.Estimator.attach('xgboost-2019-07-15-18-56-15-081')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test that everything we did so far makes sense we have create a batch transform that makes predictions on our test data. This will then be uploaded to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: xgboost-2019-07-15-18-56-15-081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................!\n"
     ]
    }
   ],
   "source": [
    "batch_input = 's3://s3.sagemaker.end2endtitanic.7ededbe5308646a3adbc/data/for_xgboost/test.csv' # The location of the test dataset\n",
    "\n",
    "# The location to store the results of the batch transform job\n",
    "batch_output = f\"s3://{PARAMETERS['bucket_name']}/batch-inference\" \n",
    "\n",
    "transformer = xgb_model.transformer(instance_count=1, instance_type='ml.m4.xlarge', output_path=batch_output)\n",
    "\n",
    "transformer.transform(data=batch_input, data_type='S3Prefix', content_type='text/csv', split_type='Line')\n",
    "\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the predictions in format required by Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv( 's3://s3.sagemaker.end2endtitanic.7ededbe5308646a3adbc/data/raw/test' ) #PARAMETERS['raw_test_data_path'])\n",
    "batch_prediction = pd.read_csv('s3://s3.sagemaker.end2endtitanic.7ededbe5308646a3adbc/batch-inference/test.csv.out',names=['Survived'])\n",
    "output = pd.DataFrame( {'PassengerId': test_data.PassengerId ,'Survived': (batch_prediction.Survived>=0.5) + 0})\n",
    "output.to_csv( 'titanic_predictions.csv', index=False)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an acuracy of 0.76076 without putting any thought into the actual model. The only data processing step we performed was label encoding the gender and the point of embarkation.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
